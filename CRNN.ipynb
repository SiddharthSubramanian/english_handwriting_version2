{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English Handwritten Text Recognition CRNN Model\n",
    "<br>\n",
    "References: \n",
    "* https://www-i6.informatik.rwth-aachen.de/publications/download/1014/VoigtlaenderPaulDoetschPatrickNeyHermann--HwritingRecognitionwithLargeMultidimensionalLongShort-TermMemoryRecurrentNeuralNetworks--2016.pdf\n",
    "* http://people.idsia.ch/~juergen/nips2009.pdf\n",
    "* https://web.stanford.edu/~jurafsky/slp3/4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'warpctc_tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f9b4a5efacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwarpctc_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warpctc_tensorflow'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from xml.etree import ElementTree\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import warpctc_tensorflow\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from utils import sparse_tuple_from as sparse_tuple_from\n",
    "from md_lstm import *\n",
    "from utils import *\n",
    "from spell import correction\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and consolidate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(data_folder_path, height_cap, width_cap, lab_length_cap):\n",
    "    \"\"\"\n",
    "    Return consolidated and preprocessed images (list) and labels (list) \n",
    "    imported from the provided locations. \n",
    "    Character mapping dictionary is also returned to decode labels. \n",
    "    \n",
    "    Arguments:\n",
    "    data_folder_path  -- Path of folder containing IAM lines folder, lines.txt,\n",
    "    testset.txt (combined testset and validationset2), trainset.txt, validationset.txt and chars.txt\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create char_map\n",
    "    with open(data_folder_path+'/chars.txt', 'rb') as f:\n",
    "        all_chars = f.read().decode('unicode_escape')\n",
    "        chars = all_chars.split()\n",
    "    char_map = {value: idx for (idx, value) in enumerate(chars)}\n",
    "\n",
    "    # Extract label text and IDs from lines.txt\n",
    "    with open(data_folder_path+'/lines.txt', 'rb') as f:\n",
    "        char = f.read().decode('unicode_escape')\n",
    "        line_raw = char[1025:].splitlines()\n",
    "    line_data = {line.split()[0] : (int(line.split()[2]), \n",
    "                                [char_map[char] for char in line.split()[8]]) for line in line_raw}\n",
    "    \n",
    "    # Extract IDs for test, train and val sets\n",
    "    with open(data_folder_path+'/trainset.txt', 'rb') as f:\n",
    "        IDs = f.read().decode('unicode_escape')\n",
    "        trainIDs = IDs.splitlines()\n",
    "    with open(data_folder_path+'/validationset.txt', 'rb') as f:\n",
    "        IDs = f.read().decode('unicode_escape')\n",
    "        valIDs = IDs.splitlines()\n",
    "    with open(data_folder_path+'/testset.txt', 'rb') as f:\n",
    "        IDs = f.read().decode('unicode_escape')\n",
    "        testIDs = IDs.splitlines()\n",
    "    \n",
    "    # For random order generation (shuffling)\n",
    "    train_p = np.random.permutation(len(trainIDs))\n",
    "    val_p = np.random.permutation(len(valIDs))\n",
    "    test_p = np.random.permutation(len(testIDs))\n",
    "    \n",
    "    # Import images and labels (map label IDs with label_dict)\n",
    "    train_images, train_labels, train_im_widths, train_lab_lengths = np.zeros(\n",
    "        shape = (len(trainIDs), height_cap, width_cap), dtype=np.float32), [None]*len(trainIDs), np.zeros(\n",
    "        shape = (len(trainIDs))), np.zeros(\n",
    "        shape = (len(trainIDs)))\n",
    "    val_images, val_labels, val_im_widths, val_lab_lengths = np.zeros(\n",
    "        shape = (len(valIDs), height_cap, width_cap), dtype=np.float32), [None]*len(valIDs), np.zeros(\n",
    "        shape = (len(valIDs))), np.zeros(\n",
    "        shape = (len(valIDs)))\n",
    "    test_images, test_labels, test_im_widths, test_lab_lengths = np.zeros(\n",
    "        shape = (len(testIDs), height_cap, width_cap), dtype=np.float32), [None]*len(testIDs), np.zeros(\n",
    "        shape = (len(testIDs))), np.zeros(\n",
    "        shape = (len(testIDs)))\n",
    "    \n",
    "    im_num, train_im_num, val_im_num, test_im_num = 0, 0, 0, 0\n",
    "    for im_path in glob.glob(data_folder_path + '/IAM_lines/*.png'):\n",
    "        im = cv2.imread(im_path, 0)\n",
    "        im_ID = im_path[im_path.rfind('/')+1:-4]\n",
    "        \n",
    "        # Threshold images to remove background and invert colors\n",
    "        im[im>line_data[im_ID][0]] = 255        \n",
    "        im = cv2.bitwise_not(im)\n",
    "        \n",
    "        im = im.astype(np.float32)\n",
    "        im = np.divide(im, 255.0)\n",
    "        \n",
    "        # Deskew image (remove slant)\n",
    "        im = bounding_box(deskew(im, get_skew_angle(im)))\n",
    "\n",
    "        # Resize - put a height cap and resize accordingly\n",
    "        out_width = int((im.shape[1]/im.shape[0])*height_cap)\n",
    "\n",
    "        if out_width < width_cap:\n",
    "            width = out_width\n",
    "            \n",
    "            im_without_pad = cv2.resize(im, (out_width, height_cap))\n",
    "            im = np.zeros((height_cap, width_cap), dtype=np.float32)\n",
    "\n",
    "            im_without_pad = im_without_pad.astype(np.float32)\n",
    "            im[:,:out_width] = im_without_pad\n",
    "        else:\n",
    "            width = width_cap\n",
    "            im = cv2.resize(im, (width_cap, height_cap))\n",
    "            \n",
    "\n",
    "        # Store image and corresponding label\n",
    "        lab = np.array(line_data[im_ID][1])\n",
    "        \n",
    "        if im_ID in trainIDs:\n",
    "            train_im_widths[train_p[train_im_num]] = width\n",
    "            train_lab_lengths[train_p[train_im_num]] = len(lab)\n",
    "            train_images[train_p[train_im_num]] = im\n",
    "            train_labels[train_p[train_im_num]] = lab\n",
    "            train_im_num+=1\n",
    "        elif im_ID in valIDs:\n",
    "            val_im_widths[val_p[val_im_num]] = width\n",
    "            val_lab_lengths[val_p[val_im_num]] = len(lab)\n",
    "            val_images[val_p[val_im_num]] = im\n",
    "            val_labels[val_p[val_im_num]] = lab\n",
    "            val_im_num+=1\n",
    "        elif im_ID in testIDs:\n",
    "            test_im_widths[test_p[test_im_num]] = width\n",
    "            test_lab_lengths[test_p[test_im_num]] = len(lab)\n",
    "            test_images[test_p[test_im_num]] = im\n",
    "            test_labels[test_p[test_im_num]] = lab\n",
    "            test_im_num+=1\n",
    "        \n",
    "        im_num+=1\n",
    "        if im_num%1000 == 0:\n",
    "            print(im_num, im_ID)\n",
    "                \n",
    "    return {'train_images' : train_images, 'train_labels' : train_labels, \n",
    "            'train_im_widths' : train_im_widths, 'train_lab_lengths' : train_lab_lengths, \n",
    "            'val_images' : val_images, 'val_labels' : val_labels,\n",
    "             'val_im_widths' : val_im_widths, 'val_lab_lengths' : val_lab_lengths,\n",
    "            'test_images' : test_images, 'test_labels' : test_labels,\n",
    "             'test_im_widths' : test_im_widths, 'test_lab_lengths' : test_lab_lengths,\n",
    "            'char_map' : char_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'readData' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fa7a43697e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight_cap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth_cap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab_length_cap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'readData' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "data = readData('./Data', height_cap=40, width_cap=800, lab_length_cap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = data['train_images']\n",
    "labels = data['train_labels']\n",
    "im_widths = data['train_im_widths']\n",
    "lab_lengths = data['train_lab_lengths']\n",
    "char_map = data['char_map']\n",
    "val_images = data['val_images']\n",
    "val_labels = data['val_labels']\n",
    "val_im_widths = data['val_im_widths']\n",
    "val_lab_lengths = data['val_lab_lengths']\n",
    "test_images = data['test_images']\n",
    "test_labels = data['test_labels']\n",
    "test_im_widths = data['test_im_widths']\n",
    "test_lab_lengths = data['test_lab_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(data, open('./Data/input_data_fixed_len_start_0_h40_w800.pickle', 'wb'))\n",
    "# pickle.dump(char_map, open('./Data/char_map_fixed_len_start_0_h40_w800.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('./Data/input_data_fixed_len_start_0_h40_w800.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_map_inv = {i:j for j,i in char_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAABaCAYAAAAvgEY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng1GpbYqsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f06f0c81a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taken|to|St|Thomas's|Hospital|it|was|his\n"
     ]
    }
   ],
   "source": [
    "# Check random image\n",
    "n = 1005\n",
    "plt.figure(figsize = (20,2))\n",
    "plt.imshow(images[n], cmap='gray')\n",
    "plt.show()\n",
    "print(''.join([char_map_inv[i] for i in labels[n]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Only keep sentences with sequence length <= im_width//8\n",
    "# indices = []\n",
    "# for idx, val in enumerate(labels):\n",
    "#     if len(val)<=(im_widths[idx]//8):\n",
    "#         indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images = images[indices]\n",
    "# labels = np.array(labels)\n",
    "# labels = labels[indices]\n",
    "# labels = list(labels)\n",
    "# lab_lengths = lab_lengths[indices]\n",
    "# im_widths = im_widths[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment and run if using Warp CTC loss - adds <BLANK> label\n",
    "char_map['<BLANK>'] = len(char_map)\n",
    "char_map_inv = {i:j for j,i in char_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config_CRNN():\n",
    "    im_height = 40\n",
    "    im_width = 800\n",
    "    lab_length_cap = 100\n",
    "    num_epochs = 350\n",
    "    batch_size = 1\n",
    "    \n",
    "    # CNN\n",
    "    conv1_patch_size = 3\n",
    "    conv2_patch_size = 3\n",
    "    conv3_patch_size = 3\n",
    "    conv4_patch_size = 3\n",
    "    conv1_depth = 12\n",
    "    conv2_depth = 24\n",
    "    conv3_depth = 48\n",
    "    conv4_depth = 96\n",
    "    \n",
    "    # RNN\n",
    "    rnn_num_hidden = 256\n",
    "    num_layers = 4\n",
    "    \n",
    "    # Number of classes\n",
    "    num_classes = 80\n",
    "    \n",
    "config_CRNN = Config_CRNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the architecture using TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CRNN(object):\n",
    "    \n",
    "    def __init__(self, config, savefile):\n",
    "        \n",
    "        self.config = config\n",
    "        self.savefile = savefile\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "    def read_and_decode(self):\n",
    "        \n",
    "        self.images = tf.placeholder(tf.float32,\n",
    "                                          shape=(None, self.config.im_height, \n",
    "                                                 self.config.im_width, 1))\n",
    "#         self.labels = tf.placeholder(tf.int32,\n",
    "#                                           shape=(self.config.batch_size, self.config.lab_length_cap))\n",
    "        self.labels_sparse = tf.sparse_placeholder(tf.int32)\n",
    "        self.lengths = tf.placeholder(tf.int32,\n",
    "                                          shape=(None))\n",
    "        self.lab_lengths = tf.placeholder(tf.int32,\n",
    "                                          shape=(None))\n",
    "        \n",
    "        self.val = tf.placeholder(tf.bool)\n",
    "    \n",
    "    def calc_ler(self, predicted, targets):\n",
    "        return tf.edit_distance(tf.cast(predicted, tf.int32), targets, normalize=True)\n",
    "    \n",
    "    def lrelu(x, alpha):\n",
    "        return tf.nn.relu(x) - alpha * tf.nn.relu(-x)\n",
    "    \n",
    "    def net(self):\n",
    "\n",
    "        # Define CNN variables\n",
    "        intitalizer = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        self.layer1_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv1_patch_size, self.config.conv1_patch_size, 1, self.config.conv1_depth]),\n",
    "                                          name='conv1_W')\n",
    "        self.layer1_biases = tf.Variable(tf.zeros([self.config.conv1_depth]), name='conv1_b')\n",
    "        self.layer2_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv2_patch_size, self.config.conv2_patch_size, \n",
    "             self.config.conv1_depth, self.config.conv2_depth]), name='conv2_W')\n",
    "        self.layer2_biases = tf.Variable(tf.zeros([self.config.conv2_depth]), name='conv2_b')\n",
    "        self.layer3_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv3_patch_size, self.config.conv3_patch_size, \n",
    "             self.config.conv2_depth, self.config.conv3_depth]), name='conv3_W')\n",
    "        self.layer3_biases = tf.Variable(tf.zeros([self.config.conv3_depth]), name='conv3_b')\n",
    "        self.layer4_weights = tf.Variable(intitalizer(\n",
    "            [self.config.conv4_patch_size, self.config.conv4_patch_size, \n",
    "             self.config.conv3_depth, self.config.conv4_depth]), name='conv4_W')\n",
    "        self.layer4_biases = tf.Variable(tf.zeros([self.config.conv4_depth]), name='conv4_b')\n",
    "        self.out_W = tf.Variable(tf.truncated_normal([2*self.config.rnn_num_hidden,\n",
    "                                                 self.config.num_classes],\n",
    "                                                stddev=0.1), name='out_W')\n",
    "        self.out_b = tf.Variable(tf.constant(0., shape=[self.config.num_classes]), name='out_b')\n",
    "\n",
    "        def model(images, seq_lengths, batch_size):\n",
    "            # ==================1==================\n",
    "            # CNN\n",
    "            with tf.name_scope('CNN_1'):\n",
    "                conv = tf.nn.conv2d(images, self.layer1_weights, [1, 1, 1, 1],\n",
    "                                    padding='SAME')\n",
    "                self.hidden_1 = lrelu(conv + self.layer1_biases)\n",
    "                conv1_out = tf.nn.max_pool(self.hidden_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                      padding='SAME')\n",
    "\n",
    "            # ==================2==================\n",
    "            # CNN\n",
    "            with tf.name_scope('CNN_2'):\n",
    "                conv = tf.nn.conv2d(conv1_out, self.layer2_weights, [1, 1, 1, 1],\n",
    "                                    padding='SAME')\n",
    "                self.hidden_2 = lrelu(conv + self.layer2_biases)\n",
    "                conv2_out = tf.nn.max_pool(self.hidden_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                      padding='SAME')\n",
    "\n",
    "            # ==================3==================\n",
    "            # CNN\n",
    "            with tf.name_scope('CNN_3'):\n",
    "                conv = tf.nn.conv2d(conv2_out, self.layer3_weights, [1, 1, 1, 1],\n",
    "                                    padding='SAME')\n",
    "                self.hidden_3 = lrelu(conv + self.layer3_biases)\n",
    "                conv3_out = tf.nn.max_pool(self.hidden_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                      padding='SAME')\n",
    "\n",
    "            # ==================4==================\n",
    "            # CNN\n",
    "            with tf.name_scope('CNN_4'):\n",
    "                conv = tf.nn.conv2d(conv3_out, self.layer4_weights, [1, 1, 1, 1],\n",
    "                                    padding='SAME')\n",
    "                self.conv4_out = lrelu(conv + self.layer4_biases)\n",
    "\n",
    "            cnn_out = tf.reduce_sum(self.conv4_out, axis=1)\n",
    "            cnn_out = tf.transpose(cnn_out, [1, 0, 2])\n",
    "            \n",
    "            # Dropout\n",
    "            if not self.val:\n",
    "                cnn_out = tf.nn.dropout(cnn_out, keep_prob=0.8)\n",
    "            \n",
    "            if self.val:\n",
    "                # Multi-Stacked RNN - reuse variables for validation set\n",
    "                with tf.variable_scope('MultiRNN', reuse=True) as scope:\n",
    "                    stacked_rnn = []\n",
    "                    for i in range(self.config.num_layers):\n",
    "                        stacked_rnn.append(tf.nn.rnn_cell.BasicLSTMCell(num_units=self.config.rnn_num_hidden,\n",
    "                                                                   state_is_tuple=True))\n",
    "                    with tf.variable_scope('forward') as fw_scope:\n",
    "                        cell_fw = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn,\n",
    "                                                              state_is_tuple=True)\n",
    "                    with tf.variable_scope('backward') as bw_scope:\n",
    "                        cell_bw = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn,\n",
    "                                                              state_is_tuple=True)\n",
    "                    self.output, self.state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        cell_fw,\n",
    "                        cell_bw,\n",
    "                        inputs=cnn_out,\n",
    "                        dtype=tf.float32,\n",
    "                        sequence_length=seq_lengths,\n",
    "                        time_major=True,\n",
    "                        scope='MultiRNN'\n",
    "                    )\n",
    "            else:\n",
    "                # Multi-Stacked RNN        \n",
    "                with tf.variable_scope('MultiRNN') as scope:\n",
    "                    stacked_rnn = []\n",
    "                    for i in range(self.config.num_layers):\n",
    "                        stacked_rnn.append(tf.nn.rnn_cell.DropoutWrapper(\n",
    "                            tf.nn.rnn_cell.BasicLSTMCell(num_units=self.config.rnn_num_hidden,\n",
    "                                                                   state_is_tuple=True),\n",
    "                            state_keep_prob=1.0,\n",
    "                            output_keep_prob=0.65,\n",
    "                            variational_recurrent=True,\n",
    "                            input_size=self.config.im_width//8,\n",
    "                            dtype = tf.float32))\n",
    "                    with tf.variable_scope('forward') as fw_scope:\n",
    "                        cell_fw = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn,\n",
    "                                                              state_is_tuple=True)\n",
    "                    with tf.variable_scope('backward') as bw_scope:\n",
    "                        cell_bw = tf.nn.rnn_cell.MultiRNNCell(stacked_rnn,\n",
    "                                                              state_is_tuple=True)\n",
    "                    self.output, self.state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                        cell_fw,\n",
    "                        cell_bw,\n",
    "                        inputs=cnn_out,\n",
    "                        dtype=tf.float32,\n",
    "                        sequence_length=seq_lengths,\n",
    "                        time_major=True,\n",
    "                        scope='MultiRNN'\n",
    "                    )\n",
    "            \n",
    "            # Fully Connected\n",
    "            with tf.name_scope('Dense'):\n",
    "                \n",
    "                self.output = tf.concat(self.output, 2)\n",
    "                \n",
    "                # Reshaping to apply the same weights over the timesteps\n",
    "                self.output = tf.reshape(self.output, [-1, 2*self.config.rnn_num_hidden])\n",
    "\n",
    "                # Doing the affine projection\n",
    "                self.logits = tf.matmul(self.output, self.out_W) + self.out_b\n",
    "\n",
    "                \n",
    "            # Reshaping back to the original shape\n",
    "            logits = tf.reshape(self.logits, [batch_size, -1, self.config.num_classes])\n",
    "\n",
    "            # Time major\n",
    "            return tf.transpose(logits, (1, 0, 2))\n",
    "        \n",
    "        \n",
    "        \n",
    "        logits = model(self.images, self.lengths, self.config.batch_size)\n",
    "        \n",
    "        # For saving the model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Training computation\n",
    "        self.loss = warpctc_tensorflow.ctc(logits, self.labels_sparse.values,\n",
    "                                           self.lab_lengths, self.lengths, self.config.num_classes-1)\n",
    "        self.cost = tf.reduce_mean(self.loss)\n",
    "        \n",
    "        # Optimizer.\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00027).minimize(self.loss)\n",
    "        \n",
    "        # Training predictions and accuracy\n",
    "        self.prediction = tf.nn.ctc_beam_search_decoder(logits, \n",
    "                                                    sequence_length=self.lengths,\n",
    "                                                   merge_repeated=True)\n",
    "        \n",
    "        # Calculate ler\n",
    "        self.ler = self.calc_ler(self.prediction[0][0], self.labels_sparse)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "                    \n",
    "        num_steps = int((self.config.num_epochs*len(images))/self.config.batch_size)\n",
    "                \n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.read_and_decode()\n",
    "        self.val = False\n",
    "        self.net()\n",
    "\n",
    "        # The op for initializing the variables.\n",
    "        init_op = tf.group(tf.global_variables_initializer(),\n",
    "                           tf.local_variables_initializer())\n",
    "        \n",
    "        # TensorBoard Summaries\n",
    "        train_cost_summary = tf.summary.scalar(\"training_cost\", self.cost)\n",
    "        train_ler_summary = tf.summary.scalar(\"training_ler\", tf.reduce_mean(self.ler))\n",
    "\n",
    "        # Convolution Summaries\n",
    "        conv_l1_W = tf.summary.histogram(\"conv_l1_W\", self.layer1_weights)\n",
    "        conv_l1_b = tf.summary.histogram(\"conv_l1_b\", self.layer1_biases)\n",
    "        conv_l1_Act = tf.summary.histogram(\"conv_l1_Act\", self.hidden_1)\n",
    "        conv_l2_W = tf.summary.histogram(\"conv_l2_W\", self.layer2_weights)\n",
    "        conv_l2_b = tf.summary.histogram(\"conv_l2_b\", self.layer2_biases)\n",
    "        conv_l2_Act = tf.summary.histogram(\"conv_l2_Act\", self.hidden_2)\n",
    "        conv_l3_W = tf.summary.histogram(\"conv_l3_W\", self.layer3_weights)\n",
    "        conv_l3_b = tf.summary.histogram(\"conv_l3_b\", self.layer3_biases)\n",
    "        conv_l3_Act = tf.summary.histogram(\"conv_l3_Act\", self.hidden_3)\n",
    "        conv_l4_W = tf.summary.histogram(\"conv_l4_W\", self.layer4_weights)\n",
    "        conv_l4_b = tf.summary.histogram(\"conv_l4_b\", self.layer4_biases)\n",
    "        conv_l4_Act = tf.summary.histogram(\"conv_l4_Act\", self.conv4_out)\n",
    "        \n",
    "        # RNN and Dense Summaries\n",
    "        rnn_out = tf.summary.histogram(\"rnn_out\", self.output)\n",
    "        dense_W = tf.summary.histogram(\"dense_W\", self.out_W)\n",
    "        dense_b = tf.summary.histogram(\"dense_b\", self.out_b)\n",
    "        dense_logits = tf.summary.histogram(\"dense_logits\", self.logits)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "                    \n",
    "            summaries = tf.summary.merge_all()\n",
    "            \n",
    "            writer = tf.summary.FileWriter(\"./TensorBoard/lines\", sess.graph)\n",
    "#             run_metadata = tf.RunMetadata()\n",
    "#             writer.add_run_metadata(run_metadata, 'compute')\n",
    "            \n",
    "            sess.run(init_op)\n",
    "            print('Initialized')\n",
    "            \n",
    "            start = time.time()\n",
    "            steps_time = start\n",
    "            epoch_time = start\n",
    "            \n",
    "            epoch = 1\n",
    "            epoch_train_cost = []\n",
    "            epoch_train_ler = []\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "\n",
    "                offset = (step * self.config.batch_size) % (len(images) - self.config.batch_size+1)\n",
    "                batch_images = images[offset:(offset + self.config.batch_size), :, :]\n",
    "                batch_images = batch_images.reshape((self.config.batch_size, self.config.im_height,\n",
    "                                             self.config.im_width, 1))\n",
    "                batch_labels = labels[offset:(offset + self.config.batch_size)]\n",
    "                batch_labels_sparse = sparse_tuple_from(batch_labels)\n",
    "                batch_lengths = im_widths[offset:(offset + self.config.batch_size)]//8\n",
    "                batch_lab_lengths = lab_lengths[offset:(offset + self.config.batch_size)]\n",
    "\n",
    "                feed_dict = {self.images : batch_images, self.labels_sparse : batch_labels_sparse,\n",
    "                             self.lengths : batch_lengths, self.lab_lengths : batch_lab_lengths}\n",
    "                \n",
    "                self.val = False\n",
    "                _, c, ler_, predictions, summ = sess.run([self.optimizer, self.cost, self.ler,\n",
    "                                                          self.prediction, summaries],\n",
    "                                                         feed_dict=feed_dict)\n",
    "                writer.add_summary(summ, step)\n",
    "                \n",
    "                epoch_train_cost.append(c)\n",
    "                epoch_train_ler.append(ler_)\n",
    "                \n",
    "                if (step % 6000 == 0):\n",
    "                    preds = np.zeros((predictions[0][0].dense_shape))\n",
    "                    i =  0\n",
    "                    for idx in predictions[0][0].indices:\n",
    "                        preds[idx[0]][idx[1]] = predictions[0][0].values[i]\n",
    "                        i+=1\n",
    "                    print('Step %d:' % (step), round(time.time() - steps_time), 'seconds')\n",
    "                    steps_time = time.time()\n",
    "                    decoded_pred = [''.join([''.join([char_map_inv[j] for j in i]) for i in preds])]\n",
    "                    print('Minibatch cost at step %d: %f' % (step, c))\n",
    "                    print('Label =', ''.join([''.join([char_map_inv[j] for j in i]) for i in batch_labels]))\n",
    "                    print('Prediction =', ''.join(decoded_pred))\n",
    "                    print('Prediction with spell check =', \n",
    "                          '|'.join([correction(word) if word != '' else '' \n",
    "                                    for word in decoded_pred[0].split('|')]))\n",
    "\n",
    "                if (step!=0 and (step+1) % int(len(images)/self.config.batch_size) == 0):\n",
    "\n",
    "                    print('Epoch', epoch, 'Completed')\n",
    "                    print('Time =', round(time.time() - epoch_time), 'seconds')\n",
    "                    \n",
    "                    # Getting validation prediction, loss and ler - passing one sasmple at a time\n",
    "                    # as CTC gives very bad results if all are passed at once (maybe due to length)\n",
    "                    epoch_val_cost = []\n",
    "                    epoch_val_ler = []\n",
    "                    for val_step in range(len(val_images)):\n",
    "                        val_offset = (val_step * self.config.batch_size) % (len(val_images) - self.config.batch_size+1)\n",
    "                        batch_val_images = val_images[val_offset:(val_offset + self.config.batch_size), :, :]\n",
    "                        batch_val_images = batch_val_images.reshape((self.config.batch_size, \n",
    "                                                                     self.config.im_height,\n",
    "                                                                     self.config.im_width, 1))\n",
    "                        batch_val_labels = val_labels[val_offset:(val_offset + self.config.batch_size)]\n",
    "                        batch_val_labels_sparse = sparse_tuple_from(batch_val_labels)\n",
    "                        batch_val_lengths = val_im_widths[val_offset:(val_offset + self.config.batch_size)]//8\n",
    "                        batch_val_lab_lengths = val_lab_lengths[val_offset:(val_offset + self.config.batch_size)]\n",
    "                        \n",
    "                        val_feed_dict = {self.images : batch_val_images, \n",
    "                                         self.labels_sparse : batch_val_labels_sparse,\n",
    "                                         self.lengths : batch_val_lengths, \n",
    "                                         self.lab_lengths : batch_val_lab_lengths}\n",
    "                        \n",
    "                        self.val = True\n",
    "                        val_cost, val_ler = sess.run([self.cost, self.ler], feed_dict=val_feed_dict)\n",
    "                        \n",
    "                        epoch_val_cost.append(val_cost)\n",
    "                        epoch_val_ler.append(val_ler)\n",
    "                        \n",
    "                    # Writing values to TensoLogs and printing epoch results\n",
    "                    epoch_val_cost, epoch_val_ler = np.mean(epoch_val_cost), np.mean(epoch_val_ler)\n",
    "                    writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"val_cost\", \n",
    "                                                                          simple_value=epoch_val_cost)]), epoch)\n",
    "                    writer.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"val_ler\", \n",
    "                                                                          simple_value=epoch_val_ler)]), epoch)\n",
    "                    \n",
    "                    print('Cost: Training =', np.mean(epoch_train_cost), \n",
    "                          'Validation =', epoch_val_cost)\n",
    "                    print('ler: Training =', np.mean(epoch_train_ler), 'Validation =', epoch_val_ler)\n",
    "                    print('-------------------------------------------------------------------------------')\n",
    "                    self.saver.save(sess, self.savefile)\n",
    "                    epoch_train_cost = []\n",
    "                    epoch_train_ler = []\n",
    "                    epoch_time = time.time()\n",
    "                    epoch+=1\n",
    "            writer.close()\n",
    "            print('Total runtime =', round(time.time() - start), 'seconds')\n",
    "            \n",
    "    def predict(self, X, X_width):\n",
    "        with tf.Session() as sess:\n",
    "            # restore the model\n",
    "            self.saver.restore(sess, self.savefile)\n",
    "            P = sess.run(self.prediction, feed_dict={self.images : X, self.lengths : X_width})\n",
    "        return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Step 0: 0 seconds\n",
      "Minibatch cost at step 0: 242.586594\n",
      "Label = Ultratoryism|,|that|the|Commons|House|upon|a\n",
      "Prediction = !z!z!z!.!.!.!u!u!o!o!ououou\n",
      "Prediction with spell check = !z!z!z!.!.!.!u!u!o!o!ououou\n",
      "Step 6000: 1922 seconds\n",
      "Minibatch cost at step 6000: 79.728615\n",
      "Label = rather|indignant|,|but|we|took|the\n",
      "Prediction = ale|toed|tod|ae|thle|he.\n",
      "Prediction with spell check = all|told|to|a|the|he\n",
      "Epoch 1 Completed\n",
      "Time = 1974 seconds\n",
      "Cost: Training = 127.348 Validation = 109.021\n",
      "ler: Training = 0.819128 Validation = 0.687795\n",
      "-------------------------------------------------------------------------------\n",
      "Step 12000: 1998 seconds\n",
      "Minibatch cost at step 12000: 0.000000\n",
      "Label = For|many|years|my|union|had|to|present|its|views\n",
      "Prediction = the|any|pon|,|wanthath|pant|th|the\n",
      "Prediction with spell check = the|any|pon|i|wanthath|part|the|the\n",
      "Epoch 2 Completed\n",
      "Time = 1958 seconds\n",
      "Cost: Training = 100.527 Validation = 92.8315\n",
      "ler: Training = 0.651759 Validation = 0.586123\n",
      "-------------------------------------------------------------------------------\n",
      "Step 18000: 1992 seconds\n",
      "Minibatch cost at step 18000: 82.334198\n",
      "Label = to|England|,|and|returned|to|Rome|for|the|winter|.|In\n",
      "Prediction = t|oghed|,|ad|shed|to|te|fe|the|dede|.|t\n",
      "Prediction with spell check = t|other|i|and|she|to|the|he|the|deed|i|t\n",
      "Epoch 3 Completed\n",
      "Time = 1953 seconds\n",
      "Cost: Training = 88.089 Validation = 84.4125\n",
      "ler: Training = 0.582054 Validation = 0.530117\n",
      "-------------------------------------------------------------------------------\n",
      "Step 24000: 1996 seconds\n",
      "Minibatch cost at step 24000: 45.904778\n",
      "Label = whole|principle|of|the|State|service|and\n",
      "Prediction = wtahe|prinelte|ot|the|whale|sersice|ald\n",
      "Prediction with spell check = the|prinelte|to|the|while|service|and\n",
      "Epoch 4 Completed\n",
      "Time = 1955 seconds\n",
      "Cost: Training = 77.4008 Validation = 72.3029\n",
      "ler: Training = 0.505536 Validation = 0.441579\n",
      "-------------------------------------------------------------------------------\n",
      "Step 30000: 1997 seconds\n",
      "Minibatch cost at step 30000: 82.805992\n",
      "Label = People|will|be|so|great|at|the|return|of|Ultratoryism|,|that|the\n",
      "Prediction = tophe|nl|he|so|pet|at|the|wton|of|lctogion|,|thet|th\n",
      "Prediction with spell check = the|no|he|so|yet|at|the|won|of|lctogion|i|the|the\n",
      "Epoch 5 Completed\n",
      "Time = 1956 seconds\n",
      "Cost: Training = 67.5218 Validation = 62.9291\n",
      "ler: Training = 0.440324 Validation = 0.384707\n",
      "-------------------------------------------------------------------------------\n",
      "Step 36000: 1999 seconds\n",
      "Minibatch cost at step 36000: 78.167221\n",
      "Label = Supplies|of|meat|and|dairy|produce|were\n",
      "Prediction = srgion|of|meat|and|onso|poscs|ner\n",
      "Prediction with spell check = surgeon|of|met|and|onto|ross|her\n",
      "Epoch 6 Completed\n",
      "Time = 1957 seconds\n",
      "Cost: Training = 60.9022 Validation = 58.8768\n",
      "ler: Training = 0.398839 Validation = 0.355239\n",
      "-------------------------------------------------------------------------------\n",
      "Step 42000: 1999 seconds\n",
      "Minibatch cost at step 42000: 45.515709\n",
      "Label = \"|Ban-the-Bomb|\"|demonstrators|.|Police|leave\n",
      "Prediction = \"|Bam-the-bont.|demonseabors.|Potice|leanre\n",
      "Prediction with spell check = i|Bam-the-bont.|demonseabors.|notice|leave\n",
      "Epoch 7 Completed\n",
      "Time = 1958 seconds\n",
      "Cost: Training = 55.9517 Validation = 55.943\n",
      "ler: Training = 0.365002 Validation = 0.332877\n",
      "-------------------------------------------------------------------------------\n",
      "Step 48000: 2000 seconds\n",
      "Minibatch cost at step 48000: 0.110154\n",
      "Label = and|shaping|your|little|craft|,|upside-down|.|For|cheapness\n",
      "Prediction = ad|tayein|por|hile|and|atletlen|to|haes\n",
      "Prediction with spell check = and|taken|for|while|and|atletlen|to|has\n",
      "Epoch 8 Completed\n",
      "Time = 1958 seconds\n",
      "Cost: Training = 52.2425 Validation = 52.3239\n",
      "ler: Training = 0.34098 Validation = 0.311672\n",
      "-------------------------------------------------------------------------------\n",
      "Step 54000: 2001 seconds\n",
      "Minibatch cost at step 54000: 76.448753\n",
      "Label = both|economic|and|political|,|and|revealed|once|again|the\n",
      "Prediction = with|ecmenie|and|pultlen|,|and|renailed|ane|agin|the\n",
      "Prediction with spell check = with|ecmenie|and|pulled|i|and|remained|and|again|the\n",
      "Epoch 9 Completed\n",
      "Time = 1960 seconds\n",
      "Cost: Training = 49.1868 Validation = 49.5948\n",
      "ler: Training = 0.322549 Validation = 0.29226\n",
      "-------------------------------------------------------------------------------\n",
      "Step 60000: 2001 seconds\n",
      "Minibatch cost at step 60000: 76.514305\n",
      "Label = South|Africa|that|Dr.|Verwoerd|is|leading|them\n",
      "Prediction = bonke|freon|thet|or|omer|d|s|leaing|then\n",
      "Prediction with spell check = bone|from|the|or|over|d|s|leaving|then\n",
      "Epoch 10 Completed\n",
      "Time = 1959 seconds\n",
      "Cost: Training = 46.7597 Validation = 46.9605\n",
      "ler: Training = 0.30562 Validation = 0.27635\n",
      "-------------------------------------------------------------------------------\n",
      "Step 66000: 2001 seconds\n",
      "Minibatch cost at step 66000: 19.959845\n",
      "Label = give|the|system|a|trial|,|adding|that|it\n",
      "Prediction = give|the|srsten|a|trial|,|acking|thatit\n",
      "Prediction with spell check = give|the|system|a|trick|i|acting|that\n",
      "Epoch 11 Completed\n",
      "Time = 1962 seconds\n",
      "Cost: Training = 44.483 Validation = 45.1838\n",
      "ler: Training = 0.294004 Validation = 0.267924\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-95d709fa251a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_CRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavefile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./saved_model/lines/model_leaky_relu_v25Aug17'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b4201500485c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 _, c, ler_, predictions, summ = sess.run([self.optimizer, self.cost, self.ler,\n\u001b[1;32m    278\u001b[0m                                                           self.prediction, summaries],\n\u001b[0;32m--> 279\u001b[0;31m                                                          feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deepLearn/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "model = CRNN(config=config_CRNN, savefile='./saved_model/lines/model_leaky_relu_v25Aug17')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_function(image, saved_model = './saved_model/lines/model_CRNN_v22Aug17_0'):\n",
    "    \n",
    "    # Prepare image\n",
    "    \n",
    "    height_cap = 40\n",
    "    width_cap = 800\n",
    "    \n",
    "    # Threshold images to remove background and invert colors\n",
    "    im = cv2.bitwise_not(image)\n",
    "    \n",
    "    im = im.astype(np.float32)\n",
    "    im = np.divide(im, 255.0)\n",
    "    \n",
    "    # Deskew image (remove slant)\n",
    "    im = bounding_box(deskew(im, get_skew_angle(im)))\n",
    "    \n",
    "    # Resize - put a height cap and resize accordingly\n",
    "    out_width = int((im.shape[1]/im.shape[0])*height_cap)\n",
    "\n",
    "    if out_width < width_cap:\n",
    "        im_widths = out_width\n",
    "\n",
    "        im_without_pad = cv2.resize(im, (out_width, height_cap))\n",
    "        im = np.zeros((height_cap, width_cap), dtype=np.float32)\n",
    "\n",
    "        im_without_pad = im_without_pad.astype(np.float32)\n",
    "        im[:,:out_width] = im_without_pad\n",
    "    else:\n",
    "        im_widths = width_cap\n",
    "        im = cv2.resize(im, (width_cap, height_cap))\n",
    "    \n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Predict\n",
    "    pred = CRNN(config=config_CRNN, savefile=saved_model)\n",
    "    pred.val_images = val_images.reshape((len(val_images), pred.config.im_height,\n",
    "                                         pred.config.im_width, 1))\n",
    "    pred.val_labels = val_labels\n",
    "    pred.val_labels_sparse = sparse_tuple_from(pred.val_labels)\n",
    "    pred.val_lab_lengths = val_lab_lengths\n",
    "    pred.val_lengths = val_im_widths//8\n",
    "    pred.read_and_decode()\n",
    "    pred.net()\n",
    "\n",
    "    im_model = np.reshape(im, (pred.config.batch_size, pred.config.im_height, pred.config.im_width, 1))\n",
    "    width_model = list(np.reshape(im_widths, (pred.config.batch_size))//8)\n",
    "\n",
    "    return ''.join([char_map_inv[i] for i in pred.predict(im_model, width_model)[0][0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(im, im_widths, saved_model = './saved_model/lines/model_CRNN_v20Aug17_0'):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict\n",
    "    pred = CRNN(config=config_CRNN, savefile=saved_model)\n",
    "    pred.val_images = val_images.reshape((len(val_images), pred.config.im_height,\n",
    "                                         pred.config.im_width, 1))\n",
    "    pred.val_labels = val_labels\n",
    "    pred.val_labels_sparse = sparse_tuple_from(pred.val_labels)\n",
    "    pred.val_lab_lengths = val_lab_lengths\n",
    "    pred.val_lengths = val_im_widths//8\n",
    "    pred.read_and_decode()\n",
    "    pred.net()\n",
    "\n",
    "    im_model = np.reshape(im, (pred.config.batch_size, pred.config.im_height, pred.config.im_width, 1))\n",
    "    width_model = list(np.reshape(im_widths, (pred.config.batch_size))//8)\n",
    "\n",
    "    return ''.join([char_map_inv[i] for i in pred.predict(im_model, width_model)[0][0][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABBCAYAAADBo1/5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACiFJREFUeJztnV2sHVUVx39/WwpajKVQm6ZtLGCTpg9a7I2UwAOSYEpj\nRBNCaIzy0OT6IKYkJIZqYvTRRKmaGEINjTygoAFi0xBrLbz4UnovlH5aKKaENpUWUz6e0Jblw+xb\nx3HOmZlz5vusXzI5Z/bsM3utPeusvWftPXtkZjiO4zjd52NNC+A4juOUgzt0x3GcnuAO3XEcpye4\nQ3ccx+kJ7tAdx3F6gjt0x3GcnjCWQ5e0UdIJSSclPVyWUI7jOE5xNOo8dEnzgNeAO4HTwAFgs5kd\nK088x3EcJy/j9NC/CJw0s7+b2b+Ap4C7yxHLcRzHKcr8MX67HHgrtn8auDmZSdI0MA2wcOHC9WvW\nrBmjSKcJZmdnAVi/fn3DkjjOZDI7O/uOmS3JyjeOQ8+Fme0AdgBMTU3ZzMxM1UU6juP0Cklv5sk3\nTsjlDLAytr8ipDmO4zgNMI5DPwCslnS9pAXAfcCucsRyHMdxijJyyMXMLkp6ANgDzAN2mtnR0iRz\nHMdxCjFWDN3MngeeL0kWx3EcZwz8SVHHiSEJSU2L4bSILtmEO/Sa6IpBTDrDHrTza1gfc060TXXe\nJlkG4Q7daSVt+zM79TF33c0MM3M7KIA7dKdxuvCHlTS09+6UQ9yZp6U7w3GH7jjO/+EOtJu0zqGn\nGZIbV79J6/l6b7hZmqz/ZNluC/lpnUN3HMdpI11oWDIduqSVkl6UdEzSUUlbQ/qPJJ2RdDBsm6oX\n13EcxxlEngeLLgIPmdnLkj4JzEraG45tN7OfVidev4iHjrrQ2heljoHDOgcnuxzq67utOelkOnQz\nOwucDd8/kHScaOnc0mkifl6H4fufa3TqdKrxspKzLZJyDJqNUYdsWWXGZZub/hn/TVKXIjoMalDz\nnLNrM4W62KAXiqFLWgXcBOwPSQ9IOiRpp6RrBvxmWtKMpJnz589nlpH3gnexsps05qrndVd9Pebm\nJFdZh/HzDyurbttLa1zyypCnvgadq0ybmTvPKOdr4r/eRf8CBRy6pKuBZ4AHzex94FHgRmAdUQ/+\nZ2m/M7MdZjZlZlNLlmSuz54sMzWtSPqg8wxLL5umeyXJnmcevUepm2QvMLmNUk7VTjwvaQ+4ZD30\nUsbTjsnedpl1Ea/bIjIOu1tJnjN+hzBKgxSXtSrijU1ctrbYXhFyOXRJVxA58yfN7FkAM3vbzC6Z\n2UfAr4leSTcyZfcE0tKHOZg6DGbY8Toal6I6jtIQ9J20UEvb66WOpy3rCFdWce7knUMdd4JVkmeW\ni4DHgeNm9kgsfVks29eBI2UIlNZKxo/NpQ3qMWWRvGBNXrxk76UK4y0j1punQUojq27b7ggHUSSM\nUUa9x22jijpL66UPu6Z55BgnTJTmZMumLw48SZ5ZLrcC3wQOSzoY0r4PbJa0DjDgFPDtMgQa9JDJ\noIubZvRFzt0UVQ+qFRlESyNZn4MG2AaVnXfgLm/+uhglFDDM+Y3rgOMdj6Stp9VbVQ1JUqY8/7Vx\n7uqqauxHvcNKNmRtsdckeWa5/BVI07hV66AXNYA29A7rMIo6phGWUU6b/iijyDDMuZZBnvrJ02su\nqltW7L7MBjAtb9UU6fi0wWdkUflLoqtkUDgmK+/cfp2Dolm3s2XLMqzMIgNs8XzJnuGgOo+fP21Q\nr03OO86o12BYb7Sqhm5Q6GOY/Q8KZWaVl3bN8lzHrPRh9V2VbaRdqzyNZFttNkmnHHrbKzOLOhx5\nnKxxhiynPqzRyyN3FwcQhzFKXY0T+so7JlSUrN8MCnuWTZYjrdqpF+2Rd8H/dMqhDyNvD6Qp0m7H\n65A1r9EWubMZtdy2XZOyabt+WQ1Smecrg6qdelbZefO2idY49KwQyDi9lbw9gToH55o2kqbLz0Ob\nwzNOtTTZQevyXWRrHHqTdPkCOk5f8Ua8OBPt0N1g2k+d18jtweli3DxOLocu6RTwAXAJuGhmU5IW\nA08Dq4jmod9rZheqEdNxHMfJosjiXF8ys3VmNhX2Hwb2mdlqYF/YH4uutYaO4/SXLvqjcd5YdDfw\nRPj+BPC18cVxHMdpni46c8gfQzfgz5IMeMzMdgBLLVorHeAfwNK0H0qaBqbD7oeSSlnzpWNcB7zT\ntBANMIl6T6LO0DO9C0yUqEvvz+TJpJxPCy43szOSPg3sBb4L7DKzRbE8F8wsdU30WJ6ZWMhmYnC9\nJ4dJ1Blc76blmCNXyMXMzoTPc8BzREvlvq2w4mL4PFeVkI7jOE42eZbPXajoXaJIWgh8mWip3F3A\n/SHb/cAfqxLScRzHySZPDH0p8FyIKc0Hfmtmf5J0APi9pC3Am8C9Oc61Y2RJu43rPTlMos7gereC\nXDF0x3Ecp/2MM23RcRzHaRHu0B3HcXpCbQ5d0kZJJySdlDT2U6VtQtJOSefic+wlLZa0V9Lr4fOa\nkC5Jvwz1cEjSF5qTfHQkrZT0oqRjko5K2hrS+673VZJekvRq0PvHIf16SfuDfk9LWhDSrwz7J8Px\nVU3KPw6S5kl6RdLusD8JOp+SdFjSQUkzIa21Nl6LQ5c0D/gVcBewluh9pGvrKLsmfgNsTKQNWhrh\nLmB12KaBR2uSsWwuAg+Z2VpgA/CdcE37rveHwB1m9nlgHbBR0gbgJ8B2M/sscAHYEvJvAS6E9O0h\nX1fZChyP7U+CzpB/2ZPmbdzMKt+AW4A9sf1twLY6yq5rI1qk7Ehs/wSwLHxfBpwI3x8DNqfl6/JG\nNG31zknSG/gE8DJwM9HTgvND+mV7B/YAt4Tv80M+NS37CLquIHJedwC7id4z3Gudg/yngOsSaa21\n8bpCLsuBt2L7p0Nanxm0NELv6iLcUt8E7GcC9A6hh4NED9PtBd4A3jWziyFLXLfLeofj7wHX1itx\nKfwc+B7wUdi/lv7rDP9d9mQ2LGMCLbbxiV4PvS7MzMI6OL1D0tXAM8CDZva+/vfVXb3U28wuAesk\nLSJ6cnpNwyJViqSvAOfMbFbS7U3LUzO3WWzZE0l/ix9sm43X1UM/A6yM7a8IaX1m0NIIvakLSVcQ\nOfMnzezZkNx7vecws3eBF4nCDYskzXWQ4rpd1jsc/xTwz5pFHZdbga8qei/CU0Rhl1/Qb52Bwsue\nNG7jdTn0A8DqMCq+ALiPaOmAPjNoaYRdwLfCiPgG4L3Y7VtnUNQVfxw4bmaPxA71Xe8loWeOpI8T\njRscJ3Ls94RsSb3n6uMe4AULAdauYGbbzGyFma0i+u++YGbfoMc6w0jLnjRv4zUOLmwCXiOKN/6g\n6cGOknX7HXAW+DdR3GwLUcxwH/A68Bdgccgrohk/bwCHgamm5R9R59uI4ouHgINh2zQBen8OeCXo\nfQT4YUi/AXgJOAn8AbgypF8V9k+G4zc0rcOY+t8O7J4EnYN+r4bt6JzfarON+6P/juM4PcGfFHUc\nx+kJ7tAdx3F6gjt0x3GcnuAO3XEcpye4Q3ccx+kJ7tAdx3F6gjt0x3GcnvAfw/sj3DUztXgAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33939a5780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im =cv2.imread('processed-strokes.png', 0)\n",
    "crop_im = im[150:200,:]\n",
    "plt.imshow(crop_im, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIEAAABaCAYAAAAvgEY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3393da0898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/lines/model_CRNN_v20Aug17_0\n",
      "So|they|proveded|to|se|if|the|coast\n"
     ]
    }
   ],
   "source": [
    "print(predict_test(test_images[10], test_im_widths[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAAzCAYAAACZgMOAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAACphJREFUeJztnW2MXFUZx3+PLYXSYl+AdCslLjQGUxO7dAjQ2BDja20M\nzBo+0JiIiaaO+oGXGFMkMcsHP2iMQRPjSLRGjVIUcUvaGETgg/FDoUunL7RUWqhQ2qVAAkS/iPr4\n4Z4zPXt77+zs3Dt3z84+v+TJnnPuvef8z9szd869e0ZUFcMwDGNwec9sCzAMwzD6izl6wzCMAccc\nvWEYxoBjjt4wDGPAMUdvGIYx4JijNwzDGHAKOXoR2Swix0TkuIhsL0uUYRiGUR7S63v0IrIA+Dvw\nSeAU8AywVVWPlCfPMAzDKEqRO/rrgeOq+qKq/hvYCdxSjizDMAyjLBYWuPYK4JUgfgq4IX2SiGwD\ntrlorUB5RsXUajUmJiZmW8bAU6vVOH36NGfOnJltKUa8vKGql/d6cd8fxqrqA6p6nape1++yZopt\n/9CZPXv2MDQ0NCUtHZ8PTE5Ooqp9Gy979uzh9OnT87Jtja75R5GLizj6V4Erg/gal2bMEmU7okaj\nQbPZbMfr9fqU+HygXq+zatUqRITR0dG+OPtGo8GuXbvsjt7oH/5OZaZGsuzzInAVsAg4AHxommvU\nW6PR0GazqWFa1RaDhqKW1q9JQ5dm6fxib6+y+zQrv360gWe2288sWtvXq69WVXq+0DnuLSRv3pwA\n7p3u/FqtpiMjI23xMQzsGDSUaa1Wq/T8Yuuzqi1d5360QavVmpdta9a1zZ6jn6nVarW28JGRkSgG\ndgwayrTQKZeVX/jhMWjt1Y1V4ehjmQ9m0VohR1/pf8aeOHGC8fFxAIaHh0vJ032z6Bm/7up1xUQv\ndWu1WlnfvHpm//79rF+/vp3X6Ohou63q9Xpp7Tad1qz6lFl+J0ZHR8+Ll11uWfPBMDKp8o5+7dq1\nWq/X259SqjolHoa7sfHx8fPymKn5PHq9vp/aetXlKbMuWbo8ReqYHgv1ej1Te6ghPB6SlZ+/NtTo\n+yatvVO7ZV1fVvvm1cHMLLD+Lt2QvFnzFHAEeA64w6WPkbxl03K2Zbq81q1b1x7Mfk0y7TT80kN6\n4Pu14iyKLldkTbCi+frr02vceWVkaQiXTML2CvPLWtvtVGavls4v3T95x7vREdah0xJGmFdWu6XH\nS9b4yWufTmMvq5w8jd2Ojbz2KbvfzAbG+u7oVwMbXPgSkoev60gc/TdmUtjixYunDHI/GcPJHf5N\nT+z0BCsy2UJLO6xQWy8PN8P6TOe4wg+wTrrC/NJr5ulr0+eUYen80m0Thn19utURtndWfbLK8OF0\n34Xp6XzSYyxMz7qJyGvTIuMubLes8VV2v5kNjFX7MBbYRbK/zRgzdPSQf/fs05vNZjvebDa10Wi0\nz/PHwngZjRjmk57kvUxqVZ2iOx1Pl+3rmeeYwnC6DfIcUxFnlFentO5036Tr3Knenerp22K6/Dul\n+7/p8rPGVbod87TntfVMxtl048LHu207s3lj1Tl6YBh4GXgviaM/CRwEdgArcq7ZBuxzdt56qU/L\nmjzpteGs88pYH4apDsKHi6zFel2edF3SZYfn+3XqrLqGx0PdWW1XVtvklZNOC9e+Z+oQ8873ZNUl\n6zlIqCFv3OTl10lXL3WaLr9Ox8vsO7OBsGocPbAUmAA+5+KrgAUk/137HWBHF3no0NCQTk5OTqlE\nVpo3VZ1yLD1B8q6bqeXRa/5ZE7rT5PblDA0N5TqCMN2fl6evU5v2Yr3kVZZzjMnKbte8cWBmlrL+\nO3rgAuAx4O6c48PA4W4cPaBjY2M9Vzh97VxyImNjY5U6vjLLUdVC/TZI5imzPaoeG2Zzzvr+MFaA\nXwH3p9JXB+G7gJ3dOno4f+20qNkkMZvLpqqZz2nMzJwVcvTT/vCIiGwC/gocAv7nkr8FbAVGnIiT\nwFdUteOuTCLSLkxVEZGOZRvGfCGchzYvjAwmtMAOwD3/wlRPhQWO3jCMc/h5aE7eyKGQoy/ywyO9\n8E/gWMVl9sJlwBuzLaILTGe5zJrOGTh4a8tymSs6rylycdWO/liRT6WqEJF9prM8TGd5zAWNYDrL\nRkT2Fbm+0k3NDMMwjOoxR28YhjHgVO3oH6i4vF4xneViOstjLmgE01k2hXRW+taNYRiGUT22dGMY\nhjHgmKM3DMMYcCpz9CKyWUSOichxEdleVbk5WnaIyFkRORykrRSRx0XkBfd3hUsXEfmR031QRDZU\npPFKEXlKRI6IyHMickekOi8SkadF5IDTeZ9Lv0pE9jo9D4nIIpd+oYsfd8eHq9AZ6F0gIvtFZHes\nOkXkpIgcEpGWf60utn53ZS8XkYdF5HkROSoiG2PSKSLXuDb09o6I3BmTxkDrXW7+HBaRB928Km9s\nFtk/oesNdZJdLk8AVwOLgAPAuirKztFzE7CBYCM24HvAdhfeDnzXhbcAfyLZ8+dGYG9FGvN+8CU2\nnQIsdeELgL2u/N8Bt7n0JvBVF/4a0HTh24CHKu77u4HfArtdPDqdJFuKXJZKi6rfXdm/BL7swouA\n5THqdOUvACaB98emEbgCeAlYHIzJL5Y5Nqtq5I3AY0H8HuCeKjs6Q9MwUx39MdxGbSRO9pgL/xTY\nmnVexXr9D75EqxO4GHgWuIHkvw0XpvufZBfUjS680J0nFelbAzwBfAzY7SZ0jDpPcr6jj6rfgWXO\nOUnMOoPyPgX8LUaNJI7+FWClG2u7gU+XOTarWrrxFfGccmkxsUrPbco2SbLfPkSg3X01u5bkbjk6\nnW45pAWcBR4n+fb2lqr+J0NLW6c7/jZwaRU6gfuBb3Juc75LI9WpwJ9FZEJEtrm02Pr9KuB14Bdu\nKexnIrIkQp2e24AHXTgqjar6KvB9kh91OkMy1iYocWzaw9gMNPmojOK9UxFZCvwBuFNV3wmPxaJT\nVf+rqiMkd8zXAx+cZUnnISKfBc6q6sRsa+mCTaq6AfgM8HURuSk8GEm/LyRZ/vyJql4L/ItkGaRN\nJDpxa9s3A79PH4tBo3tGcAvJh+f7gCXA5jLLqMrRvwpcGcTXuLSYeE1EVgO4v2dd+qxpF5ELSJz8\nb1T1kVh1elT1LeApkq+Zy0XE76UUamnrdMeXAW9WIO8jwM0ichLYSbJ888MIdfo7PFT1LPBHkg/P\n2Pr9FHBKVfe6+MMkjj82nZB8YD6rqq+5eGwaPwG8pKqvq+q7wCMk47W0sVmVo38G+IB7iryI5GvU\noxWV3S2PAre78O0ka+I+/QvuifyNwNs6zb77ZSAiAvwcOKqqP4hY5+UistyFF5M8RzhK4vBvzdHp\n9d8KPOnuqvqKqt6jqmtUdZhk/D2pqp+PTaeILBGRS3yYZG35MJH1u6pOAq+IiN9V8ePAkdh0OrZy\nbtnGa4lJ48vAjSJysZv3vi3LG5sVPgzZQvLmyAng3qrKzdHyIMla2LskdyZfIlnjegJ4AfgLsNKd\nK8CPne5DwHUVadxE8pXyINBytiVCnR8G9judh4Fvu/SrgaeB4yRfmS906Re5+HF3/OpZ6P+Pcu6t\nm6h0Oj0HnD3n50ps/e7KHgH2ub4fB1bEppNkGeRNYFmQFpVGV/Z9wPNuDv0auLDMsWlbIBiGYQw4\n9jDWMAxjwDFHbxiGMeCYozcMwxhwzNEbhmEMOOboDcMwBhxz9IZhGAOOOXrDMIwB5/8B3YXIqcxa\nkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f33c25bbd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./saved_model/lines/model_CRNN_v20Aug17_0\n",
      "therl|to|a|wait|the|erviwval|of\n"
     ]
    }
   ],
   "source": [
    "print(predict_function(crop_im))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
